#Researchers collected measurements from loblolly pines.
#Calculate the mean squared errors for 
#-fold cross-validation with 
# = length of the training dataset. Use the negative of the cross_val_score() method with the following parameters, respectively:
#the model variable
#x_variable
#y_variable
#the scoring parameter set to neg_mean_squared_error
#the cv parameter set to len(x_variable)

# Import packages and functions
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split, cross_val_score

rng = np.random.RandomState(36)

# Load the dataset
loblollyPine = pd.read_csv('loblollyPineSample.csv')

# Split dataset into training data and testing data
trainingDataName, testingDataName = train_test_split(loblollyPine, test_size=0.3, random_state=rng)

# Store relevant columns as variables
X = trainingDataName[['age']]
y = trainingDataName[['height']]

# Initialize the model -- quadratic polynomial regression model
polyFeatures = PolynomialFeatures(degree=2, include_bias=False)
xPoly = polyFeatures.fit_transform(X)
quadPolyRegModel = LinearRegression()

# Evaluate accuracy
# neg_mean_square_error is the negative MSE, so append a - so the scores are positive.
tenFoldScores = -cross_val_score(quadPolyRegModel, X, y, scoring='neg_mean_squared_error', cv=10)

print('ten-fold average MSE =', np.mean(tenFoldScores), '\n', tenFoldScores)

# neg_mean_square_error is the negative MSE, so append a - so the scores are positive.
LOOCVScores = -cross_val_score(quadPolyRegModel, X,y, scoring = 'neg_mean_squared_error',cv = len(X))

print('\nk-fold average MSE =', np.mean(LOOCVScores), '\n', LOOCVScores)



#output
#ten-fold average MSE = 10.526835026608941 
#[10.32884027 16.02918445 14.80637844 16.70004643 10.71253826  4.46552235
 # 6.19443078  4.18811203  6.01761972 15.82567754]

#k-fold average MSE = 10.840972663659088 
# [8.68313585e+00 8.09899658e+00 1.62352502e+01 8.49726814e+00
# 6.14413277e+00 4.68566230e+01 9.06351833e+00 1.16479580e+01
# 1.67389115e+01 3.98143618e-02 5.05655360e-01 3.86913416e+01
# 4.12108702e+01 2.01394722e-01 1.26467547e+00 1.16815280e+01
# 1.10169834e+01 1.06877950e+01 1.98302731e+01 3.13278804e+00
# 5.13211724e+00 3.43191651e+00 3.37764415e+00 1.07236058e+01
# 5.72805261e+00 3.23059711e+00 1.85594541e+00 3.29839373e+00
# 8.92210238e+00 2.89547215e+00 1.22302310e+01 7.76196748e-01
# 1.51184697e+01 7.70060776e-01 3.17143243e+01]
